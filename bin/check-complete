#!/usr/bin/env python3
"""LICENSE
Copyright 2017 Hermann Krumrey <hermann@krumreyh.com>

This file is part of photo-sorter.

photo-sorter is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

photo-sorter is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with photo-sorter.  If not, see <http://www.gnu.org/licenses/>.
LICENSE"""

import os
import json
import hashlib
import argparse
from typing import List, Tuple
from colorama import Fore, Style
from subprocess import check_output


CACHE = {}
"""
Caches hashes so they don't have to be recalculated every time
"""


def main():
    """
    The main function of this script
    Traverses a given directory and finds any duplicate files based on their
    SHA1 hash
    :return: None
    """

    parser = argparse.ArgumentParser()
    parser.add_argument("sources", nargs=2, help="The directories to check")
    one, two = parser.parse_args().sources

    hash_data = {}
    hashes = {}

    print("Calculating Hashes...")
    hash_data[one] = load_checksums(one)
    hashes[one] = list(map(lambda x: x[1], hash_data[one]))
    hash_data[two] = load_checksums(two)
    hashes[two] = list(map(lambda x: x[1], hash_data[two]))
    print("Calculated Hashes")

    for source, dest in [(one, two), (two, one)]:
        missing = []
        for file, _hash in hash_data[source]:
            if _hash not in hashes[dest]:
                missing.append(file)

        print("{} is missing the following files:".format(dest))
        for file in missing:
            print("{}{}{}".format(Fore.LIGHTYELLOW_EX, file, Style.RESET_ALL))


def load_checksums(directory: str) -> List[Tuple[str, str]]:
    """
    Loads the checksums of all files in the directory recursively
    :param directory: The directory to traverse
    :return: A list of tuples pairing file paths with SHA1 hashes
    """

    hash_data = []

    for child in os.listdir(directory):
        child_path = os.path.join(directory, child)

        if os.path.isfile(child_path):
            checksum = sha1(child_path)
            hash_data.append((child_path, checksum))
        else:
            hash_data += load_checksums(child_path)

    return hash_data


def sha1(file: str) -> str:
    """
    Calculates the SHA1 hash of a file
    :param file: The path to the file
    :return: The SHA1 hash
    """
    old_hash = CACHE.get(file)

    if old_hash is None:
        print("Calculating hash for {}".format(file))
        try:
            with open(file, "rb") as fil:
                content = fil.read()
            new_hash = hashlib.sha1(content).hexdigest()
        except MemoryError:
            output = check_output(["sha1sum", file]).decode("utf-8")
            new_hash = output.split(" ")[0]
        CACHE[file] = new_hash
        return new_hash

    else:
        print("Using old hash for {}".format(file))
        return old_hash


if __name__ == "__main__":
    if os.path.isfile("hash-cache.json"):
        with open("hash-cache.json", "r") as f:
            CACHE = json.load(f)

    # noinspection PyBroadException
    try:
        main()
    except BaseException:
        with open("hash-cache.json", "w") as f:
            json.dump(CACHE, f)
